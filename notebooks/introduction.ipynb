{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark with Jupyter\n",
    "\n",
    "PySpark is an interface into the Apache Spark framework:\n",
    "\n",
    "> Apache Spark is an open-source cluster computing framework originally developed in the AMPLab at UC Berkeley. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's in-memory primitives provide performance up to 100 times faster for certain applications.\n",
    "\n",
    "Spark is used for big data applications since, by definition, they are not able to be processed within a single compute resource.  A common use for the framework is to process large amounts of data and use Machine Learning techniques to analyze, understand, and predict outcomes for external processes.\n",
    "\n",
    "This notebook was created by aggregating information from various sources, including notebooks and code that I have developed on projects, but also using some of the following books:, [Learning Spark](http://shop.oreilly.com/product/0636920028512.do), [Advanced Analytics with Spark](http://shop.oreilly.com/product/0636920035091.do), and [High Performance Spark](http://shop.oreilly.com/product/0636920046967.do)\n",
    "\n",
    "Some of these resources do not include Python or PySpark usage directly, but I have been able to translate the information into Pythonic, or at least Python, for use here.\n",
    "\n",
    "In addition, many resources exist on the web for exploring [Python](https://www.python.org/) and [PySpark](http://spark.apache.org/docs/latest/api/python/index.html) as well as Machine Learning and other big data uses in general.  Due to the dynamic nature of these resources, you should always search and use the most current information available at the time you need it.\n",
    "\n",
    "## Import the module\n",
    "\n",
    "This is already installed in the docker container, so simply import it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark Context\n",
    "\n",
    "Creating a SparkContext requires the configuration for Spark operation to be defined.  This is most easily done by creating a SparkConf object with the desired parameter values for the way you want Spark to operate.  Here we define a 'local' style operation since we want to explore Spark and PySpark without needing to have a cluster available for job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "[('spark.master', 'local[*]'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.app.name', 'Introduction Notebook')]\n"
     ]
    }
   ],
   "source": [
    "# Create a simple local Spark configuration.\n",
    "conf = (\n",
    "    pyspark\n",
    "      .SparkConf()\n",
    "      .setMaster('local[*]')\n",
    "      .setAppName('Introduction Notebook')\n",
    ")\n",
    "\n",
    "# Show the configuration:\n",
    "import pprint as pp\n",
    "print('Configuration:')\n",
    "pp.pprint(conf.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creating a context should only be done once per session.  Guarding the creation with the \"try\" block ensures that we will only create the context the first time the following cell is executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  1.6.1\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark context for local work.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    sc = pyspark.SparkContext(conf = conf)\n",
    "\n",
    "# Check that we are using the expected version of PySpark.\n",
    "print('Version: ',sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prove the module is available\n",
    "\n",
    "Create a simple example and execute it in order to demonstrate that the module working correctly and the context is configured correctly.\n",
    "\n",
    "The following creates an RDD initialized with a range of numbers, then samples 5 of them.  Spark will have distributed the RDD data and the work execution among the available executors in order to perform this processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 randomly selected values from the range: [379, 848, 521, 851, 596]\n"
     ]
    }
   ],
   "source": [
    "# Prove that Spark is installed and working correctly\n",
    "rdd = sc.parallelize(range(1000))\n",
    "result = rdd.takeSample(False, 5)\n",
    "print('5 randomly selected values from the range: %s' % result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Counting using PySpark with Jupyter\n",
    "\n",
    "The common introduction to large scale distributed computing is to perform a word count.\n",
    "\n",
    "We do that here.  :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Local Spark Context\n",
    "\n",
    "Configure and create a Spark Context object in order to work with Spark in this notebook.  See the _Introduction_ notebook for more detail about these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# Local Spark configuration\n",
    "conf = (\n",
    "    pyspark\n",
    "      .SparkConf()\n",
    "      .setMaster('local[*]')\n",
    "      .setAppName('Word Counting Notebook')\n",
    ")\n",
    "\n",
    "\n",
    "# Create a Spark context for local work of has not already been done.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    sc = pyspark.SparkContext(conf = conf)\n",
    "\n",
    "# Check that we are using the expected version of PySpark.\n",
    "print('Version: %s' % sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Some Text\n",
    "\n",
    "Once we have a working Spark instance, we can perform some actual work.  For the following, the full text for Shakespeare's _The Taming of the Shrew_ was obtained and will be processed.  The text was obtained from [lexically.net](http://lexically.net/wordsmith/support/shakespeare.html) which obtained the actual corpus from the [Online Library of Liberty](http://www.lexically.net).\n",
    "\n",
    "If you download and process these files yourself, note that they are stored in 16 bit Unicode.  For simplicity, there is a local copy of this one play located in the _data_ directory that is stored in UTF8 format.\n",
    "\n",
    "Here we are creating a Spark Resilient Distributed Dataset ([RDD](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds)) object.  This is a very common way to store data to be manipulated with Spark.  It is an immutable stream of data that can be processed in order, and since it is immutable, Spark can partition it into chunks to distribute among the available processing resources when it is time to process.  RDDs will in general work best if they are sized to fit into memory.\n",
    "\n",
    "The Spark Context _textFile_ method creates a new RDD loaded with the contents of a text file with each element of the RDD corresponding to a line in the original text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTF8 encoded textfile.\n",
    "shrewText = sc.textFile(\"data/tamingoftheshrew.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most text processing and NLP work flows, removal of stop words reduces the size of the required tasks.  We can use the standard stop words available from the _stopwords_  Python package for our list of words to remove from the corpus.  These stop words will be needed by each worker in a distributed processing environment.\n",
    "\n",
    "Spark includes the concept of a [_broadcast_](http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables) variable.  This is a shared variable that Spark will make available to each distributed worker by copying the data as efficiently as possible to the destination workers.  Each host which executes worker processes will then have the data available without any additional copy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab stop words to remove from the corpus.\n",
    "from stop_words import get_stop_words\n",
    "stopwords = sc.broadcast( set(get_stop_words('en')) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are processing input lines, we remove multiple whitespace portions from each line.  These are found and replaced using a regular expression pattern.  This pattern was broadcast to all worker processes since the pattern itself is immutable and common to all workers.\n",
    "\n",
    "In addition to the whitespace, we find what appear to be HTML element tags in the file.  These indicate speaker and other non-verbal information for the play.  We can remove these using a regular expression as well.  And this regular expression is broadcast also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regular expression to find (any number of) spaces.\n",
    "import re\n",
    "multispace = sc.broadcast( re.compile(r'\\s+') )\n",
    "elementtag = sc.broadcast( re.compile(r'<[^>]*>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus to process and some immutable data to work with, we can start working on the data. First, we split the input into individual words. We can easily do this by splitting on whitespace of any kind and size, then creating an output record for each word resulting from the split.\n",
    "\n",
    "In the code below, the splitting is done internal to the flatMap call. In that call, each line is processed to replace whitespace of any kind with a single space, all text is converted to lower case for counting, and then the single spaces are used to split the line into a record for each word.\n",
    "\n",
    "Once the words have been converted to records, the stop words are removed and any remaining empty records are removed.  Note that use of the broadcast variables requires that the _.value_ attribute be accessed to obtain the original variable from the broadcast variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split a line into words at each space.\n",
    "split_line = lambda line: normalize_line(line).split(\" \")\n",
    "\n",
    "# Split the input and filter out the undesireable parts.\n",
    "shrewWords  = (\n",
    "    shrewText.flatMap(split_line)\n",
    "      .filter(lambda w: len(w) > 0)               # Remove empty words.\n",
    "      .filter(lambda w: w not in stopwords.value) # Remove stop words.\n",
    ")\n",
    "\n",
    "# Remove outside whitespace, convert to lower case, collapse remaining whitespace to single spaces.\n",
    "def normalize_line(line):\n",
    "    normal_line = line.strip().lower()  # Remove surrounding spaces, make lower case.\n",
    "    normal_line = elementtag.value.sub(' ',normal_line) # Remove element tags.\n",
    "    normal_line = multispace.value.sub(' ',normal_line) # Replace multi-spaces with single spaces.\n",
    "    return(normal_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a record for each individual word in the corpus, we can group and count them.  To do this we create Key/Value records by mapping the words.  The Key is set to the word, and the Value is given an integer value of 1.\n",
    "\n",
    "Calling _.reduceByKey()_ on these Key/Value records groups the records for each word together and processes then using the provided function.  Here we add up all of the individual Values for the records of that Key.  Since each word in the corpus started with a Value of 1, adding these together results in the count of the number of times that (Key) word appears in the corpus.\n",
    "\n",
    "We go on to sort the result in descending order by count, then save the result.  This is done by writing the RDD to a text file.\n",
    "\n",
    "The saved result is stored in parts by the RDD and will need to be combined in order to see the entire output together.  Other storage types can provide a single output file for review.  The output will be in a directory with the location name, and the partition files are located there along with some metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the words by mapping a value for each row\n",
    "# and adding the values up for each unique key.\n",
    "shrewCounts = (\n",
    "    shrewWords.map(lambda word: (word, 1)) # Generate the Key/Value records.\n",
    "      .reduceByKey(lambda x, y: x + y)     # Generate the word counts.\n",
    "      .map(lambda t: (t[1],t[0]))          # Swap Key and Value to sort by Value.\n",
    "      .sortByKey(False)                    # Sort in descending order.\n",
    "      .map(lambda t: (t[1],t[0]))          # Swap back to original sense of Key/Value.\n",
    ")\n",
    "\n",
    "resultsLocation = 'shrewcounts'\n",
    "\n",
    "# Ensure that there is no previous output in the location.\n",
    "# Choose to store multiple results by using multiple locations.\n",
    "import shutil\n",
    "shutil.rmtree(resultsLocation,ignore_errors=True)\n",
    "\n",
    "# Store the results\n",
    "shrewCounts.saveAsTextFile(resultsLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can collect up and display interesting information about the processing results.  While these results appear expected -- Shakespeare would certainly have been filled with the use of words like _thou_ and _shall_, we can see that ETL processing is not complete for this corpus.  Here the _'tis_ word demonstrates that we have not dealt with punctuation marks.  If you examine the result file(s) created above, you will see many examples of punctuation and other symbols in the counted words.  Indeed, Shakespeare is full of 'em."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  4845 , Total words:  11257 \n",
      "\n",
      "will :  146\n",
      "thou :  112\n",
      "shall :  99\n",
      "thy :  85\n",
      "good :  76\n",
      "sir, :  74\n",
      "you, :  67\n",
      "why, :  55\n",
      "'tis :  52\n",
      "let :  49\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique words and the total number of words.\n",
    "# Sans stop words, of course.\n",
    "countOfUniqueWords = shrewCounts.count()\n",
    "totalCountOfWords  = shrewCounts.map(lambda t: t[1]).reduce(lambda x,y: x+y)\n",
    "\n",
    "# Look at some results.\n",
    "print('Unique words: ',countOfUniqueWords,', Total words: ',totalCountOfWords,'\\n')\n",
    "for k,v in shrewCounts.take(10):\n",
    "    print(k,': ',v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
